#!/usr/bin/env python3
"""
ç”Ÿæˆæ··æ·†çŸ©é˜µå’Œè¯¦ç»†è¯„ä¼°æŒ‡æ ‡
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
from datasets import load_dataset
from torch.utils.data import DataLoader
import os

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'PingFang SC', 'Hiragino Sans GB', 'STSong', 'SimHei']
plt.rcParams['axes.unicode_minus'] = False

def load_test_data():
    """åŠ è½½æµ‹è¯•æ•°æ®"""
    from train import TrashDataset, get_transforms
    
    # åŠ è½½æ•°æ®é›†
    dataset = load_dataset("garythung/trashnet")
    
    # è·å–éªŒè¯å˜æ¢
    _, val_transform = get_transforms()
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
    test_dataset = TrashDataset(dataset['train'], transform=val_transform)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
    
    return test_loader, test_dataset.dataset.features['label'].names

def evaluate_model():
    """è¯„ä¼°æ¨¡å‹å¹¶ç”Ÿæˆå›¾è¡¨"""
    from model_utils import load_model, get_device
    from app import create_model
    
    # åŠ è½½æ¨¡å‹
    device = get_device()
    model = create_model(6)
    model, class_names = load_model(model, 'models/trash_classifier_final.pth')
    model.eval()
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_loader, label_names = load_test_data()
    
    # è¿›è¡Œé¢„æµ‹
    all_predictions = []
    all_labels = []
    
    print("ğŸ” æ­£åœ¨è¯„ä¼°æ¨¡å‹...")
    with torch.no_grad():
        for batch_idx, (data, target) in enumerate(test_loader):
            data, target = data.to(device), target.to(device)
            output = model(data)
            pred = output.argmax(dim=1)
            
            all_predictions.extend(pred.cpu().numpy())
            all_labels.extend(target.cpu().numpy())
            
            if batch_idx % 10 == 0:
                print(f"å·²å¤„ç† {batch_idx * len(data)} å¼ å›¾ç‰‡...")
    
    # ç”Ÿæˆæ··æ·†çŸ©é˜µ
    cm = confusion_matrix(all_labels, all_predictions)
    
    # åˆ›å»ºresults/chartsç›®å½•
    os.makedirs('results/charts', exist_ok=True)
    
    # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=class_names, yticklabels=class_names)
    plt.title('åƒåœ¾åˆ†ç±»æ¨¡å‹æ··æ·†çŸ©é˜µ', fontsize=16, fontweight='bold')
    plt.xlabel('é¢„æµ‹ç±»åˆ«', fontsize=12)
    plt.ylabel('çœŸå®ç±»åˆ«', fontsize=12)
    plt.tight_layout()
    plt.savefig('results/charts/confusion_matrix.png', dpi=300, bbox_inches='tight', facecolor='white')
    print(f"âœ… æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ°: results/charts/confusion_matrix.png")
    plt.close()
    
    # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
    report = classification_report(all_labels, all_predictions, 
                                 target_names=class_names, output_dict=True)
    
    # ç»˜åˆ¶è¯¦ç»†æŒ‡æ ‡å›¾
    metrics = ['precision', 'recall', 'f1-score']
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    for i, metric in enumerate(metrics):
        values = [report[class_name][metric] for class_name in class_names]
        bars = axes[i].bar(class_names, values, alpha=0.8, color=plt.cm.viridis(np.linspace(0, 1, len(class_names))))
        axes[i].set_title(f'{metric.capitalize()}', fontsize=14, fontweight='bold')
        axes[i].set_ylabel(metric.capitalize())
        axes[i].set_ylim(0, 1)
        axes[i].tick_params(axis='x', rotation=45)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars, values):
            axes[i].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                        f'{value:.3f}', ha='center', va='bottom', fontweight='bold')
    
    plt.suptitle('åˆ†ç±»è¯¦ç»†æŒ‡æ ‡', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig('results/charts/detailed_metrics.png', dpi=300, bbox_inches='tight', facecolor='white')
    print(f"âœ… è¯¦ç»†æŒ‡æ ‡å›¾å·²ä¿å­˜åˆ°: results/charts/detailed_metrics.png")
    plt.close()
    
    # æ‰“å°æ€»ä½“å‡†ç¡®ç‡
    accuracy = np.sum(np.array(all_predictions) == np.array(all_labels)) / len(all_labels)
    print(f"\nğŸ“Š æ¨¡å‹æ€»ä½“å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy*100:.2f}%)")

if __name__ == "__main__":
    evaluate_model()
