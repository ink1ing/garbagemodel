# æ ¸å¿ƒæ–‡ä»¶: æ˜¯ - æ¨¡å‹è®­ç»ƒä¸»è„šæœ¬
# å¿…é¡»: æ˜¯ - ç”¨äºè®­ç»ƒåƒåœ¾åˆ†ç±»æ¨¡å‹

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import models, transforms
from torch.utils.data import DataLoader, random_split
from datasets import load_dataset
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from font_config import setup_chinese_font

# è®¾ç½®ä¸­æ–‡å­—ä½“
setup_chinese_font()
import numpy as np
from model_utils import save_model, get_device, ProgressBar  # å¯¼å…¥ProgressBarç±»
import argparse

# è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡ç°
torch.manual_seed(42)
np.random.seed(42)

# è‹±æ–‡å›¾è¡¨ç”Ÿæˆå‡½æ•°
def setup_english_plotting():
    """Setup English plotting environment"""
    plt.style.use('default')
    plt.rcParams['font.family'] = 'sans-serif'
    plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans', 'Liberation Sans']
    plt.rcParams['font.size'] = 10
    plt.rcParams['axes.unicode_minus'] = False

def plot_english_training_results(train_losses, train_accs, val_losses, val_accs, save_path="training_results_english.png"):
    """Plot training results with English labels"""
    setup_english_plotting()
    
    epochs = range(1, len(train_losses) + 1)
    
    # Create comprehensive training results plot
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('Trash Classification Model Training Results', fontsize=16, fontweight='bold')
    
    # Plot 1: Training and Validation Loss
    ax1.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2, marker='o', markersize=4)
    ax1.plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2, marker='s', markersize=4)
    ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Plot 2: Training and Validation Accuracy
    ax2.plot(epochs, train_accs, 'g-', label='Training Accuracy', linewidth=2, marker='o', markersize=4)
    ax2.plot(epochs, val_accs, 'orange', label='Validation Accuracy', linewidth=2, marker='s', markersize=4)
    ax2.set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # Highlight best performance
    if val_accs:
        best_val_epoch = np.argmax(val_accs) + 1
        best_val_acc = max(val_accs)
        ax2.plot(best_val_epoch, best_val_acc, 'r*', markersize=15)
        ax2.annotate(f'Best: {best_val_acc:.4f}', 
                     xy=(best_val_epoch, best_val_acc),
                     xytext=(best_val_epoch + 1, best_val_acc - 0.02),
                     arrowprops=dict(facecolor='red', shrink=0.05),
                     fontsize=12, fontweight='bold')
    
    # Plot 3: Loss Comparison Bar Chart (last 10 epochs)
    recent_epochs = epochs[-min(10, len(epochs)):]
    recent_train_losses = train_losses[-min(10, len(train_losses)):]
    recent_val_losses = val_losses[-min(10, len(val_losses)):]
    
    x = np.arange(len(recent_epochs))
    width = 0.35
    ax3.bar(x - width/2, recent_train_losses, width, label='Training Loss', alpha=0.8)
    ax3.bar(x + width/2, recent_val_losses, width, label='Validation Loss', alpha=0.8)
    ax3.set_title('Recent Loss Comparison (Last 10 Epochs)', fontsize=14, fontweight='bold')
    ax3.set_xlabel('Epoch')
    ax3.set_ylabel('Loss')
    ax3.set_xticks(x)
    ax3.set_xticklabels(recent_epochs)
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # Plot 4: Accuracy Improvement with Fill
    ax4.plot(epochs, train_accs, 'g-', label='Training Accuracy', linewidth=3, alpha=0.7)
    ax4.plot(epochs, val_accs, 'orange', label='Validation Accuracy', linewidth=3, alpha=0.7)
    ax4.fill_between(epochs, train_accs, alpha=0.3, color='green')
    ax4.fill_between(epochs, val_accs, alpha=0.3, color='orange')
    ax4.set_title('Accuracy Improvement Over Time', fontsize=14, fontweight='bold')
    ax4.set_xlabel('Epoch')
    ax4.set_ylabel('Accuracy')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"âœ… English training results chart saved: {save_path}")
    plt.close()

def plot_english_final_summary(train_losses, train_accs, val_losses, val_accs, final_epoch, save_path="final_summary_english.png"):
    """Plot final training summary with English labels"""
    setup_english_plotting()
    
    # Create summary plot
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('Trash Classification Model - Final Training Summary', fontsize=16, fontweight='bold')
    
    # Final performance metrics
    final_metrics = [train_accs[-1], val_accs[-1], 1 - train_accs[-1], 1 - val_accs[-1]]
    labels = ['Train Accuracy', 'Val Accuracy', 'Train Error', 'Val Error']
    colors = ['#2E8B57', '#FF6347', '#98FB98', '#FFA07A']
    
    ax1.pie(final_metrics, labels=labels, colors=colors, autopct='%1.2f%%', startangle=90)
    ax1.set_title(f'Final Performance (Epoch {final_epoch})', fontsize=14)
    
    # Training progression
    epochs = range(1, len(train_accs) + 1)
    ax2.plot(epochs, val_accs, 'o-', linewidth=3, markersize=6, color='#4169E1', label='Validation Accuracy')
    ax2.plot(epochs, train_accs, 's-', linewidth=2, markersize=4, color='#32CD32', alpha=0.7, label='Training Accuracy')
    ax2.set_title('Accuracy Progression', fontsize=14)
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # Loss progression
    ax3.plot(epochs, val_losses, 'o-', linewidth=3, markersize=6, color='#DC143C', label='Validation Loss')
    ax3.plot(epochs, train_losses, 's-', linewidth=2, markersize=4, color='#FF69B4', alpha=0.7, label='Training Loss')
    ax3.set_title('Loss Progression', fontsize=14)
    ax3.set_xlabel('Epoch')
    ax3.set_ylabel('Loss')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # Model performance statistics
    stats = {
        'Final Train Acc': train_accs[-1],
        'Final Val Acc': val_accs[-1],
        'Best Val Acc': max(val_accs),
        'Final Train Loss': train_losses[-1],
        'Final Val Loss': val_losses[-1],
        'Min Val Loss': min(val_losses)
    }
    
    y_pos = np.arange(len(stats))
    values = [round(v, 4) for v in stats.values()]
    bars = ax4.barh(y_pos, values, color=['#87CEEB', '#98FB98', '#FFB6C1', '#DDA0DD', '#F0E68C', '#FFA07A'])
    ax4.set_yticks(y_pos)
    ax4.set_yticklabels(stats.keys())
    ax4.set_title('Training Statistics Summary', fontsize=14)
    ax4.set_xlabel('Value')
    
    # Add value labels on bars
    for i, bar in enumerate(bars):
        width = bar.get_width()
        ax4.text(width + 0.01, bar.get_y() + bar.get_height()/2, 
                f'{values[i]:.4f}', ha='left', va='center', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"âœ… English final summary chart saved: {save_path}")
    plt.close()

# æ•°æ®é¢„å¤„ç†å’Œå¢å¼º
def get_transforms():
    train_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(10),
        transforms.ColorJitter(brightness=0.2, contrast=0.2),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    val_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    return train_transform, val_transform

# æ•°æ®é›†åŠ è½½å’Œå¤„ç†ç±»
class TrashDataset(torch.utils.data.Dataset):
    def __init__(self, dataset, transform=None):
        self.dataset = dataset
        self.transform = transform

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        item = self.dataset[idx]
        image = item['image']
        label = item['label']

        if self.transform:
            image = self.transform(image)

        return image, label

# åŠ è½½æ•°æ®é›†å¹¶åˆ›å»ºæ•°æ®åŠ è½½å™¨
def prepare_data(batch_size=32, val_split=0.2):
    print("æ­£åœ¨åŠ è½½æ•°æ®é›†...")

    # åˆ›å»ºæ•°æ®é›†ç¼“å­˜ç›®å½•
    cache_dir = os.path.join(os.getcwd(), 'dataset_cache')
    if not os.path.exists(cache_dir):
        os.makedirs(cache_dir)
        print(f"å·²åˆ›å»ºæ•°æ®é›†ç¼“å­˜ç›®å½•: {cache_dir}")
    else:
        print(f"ä½¿ç”¨æœ¬åœ°ç¼“å­˜æ•°æ®é›†: {cache_dir}")

    # ä½¿ç”¨ç¼“å­˜ç›®å½•åŠ è½½æ•°æ®é›†
    dataset = load_dataset("garythung/trashnet", cache_dir=cache_dir)
    train_dataset = dataset['train']
    print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆï¼Œå…± {len(train_dataset)} ä¸ªæ ·æœ¬")

    # è·å–åˆ†ç±»æ ‡ç­¾
    labels = train_dataset.features['label'].names
    num_classes = len(labels)
    print(f"åˆ†ç±»ç±»åˆ«: {labels}")

    # è·å–æ•°æ®è½¬æ¢
    train_transform, val_transform = get_transforms()

    # è®¡ç®—åˆ†å‰²å¤§å°
    dataset_size = len(train_dataset)
    val_size = int(dataset_size * val_split)
    train_size = dataset_size - val_size

    # åˆ†å‰²æ•°æ®é›†
    train_subset, val_subset = random_split(train_dataset, [train_size, val_size])

    # åˆ›å»ºæ•°æ®é›†å®ä¾‹
    train_ds = TrashDataset(train_subset, transform=train_transform)
    val_ds = TrashDataset(val_subset, transform=val_transform)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    # é’ˆå¯¹M3 Proä¼˜åŒ–å·¥ä½œçº¿ç¨‹æ•°(12æ ¸å¿ƒCPU)
    num_workers = min(8, os.cpu_count() or 4)  # ä½¿ç”¨å¯ç”¨CPUæ ¸å¿ƒæ•°ï¼Œä½†æœ€å¤š8ä¸ª
    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, 
                             num_workers=num_workers, pin_memory=True)
    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, 
                           num_workers=num_workers, pin_memory=True)

    print(f"è®­ç»ƒé›†å¤§å°: {len(train_subset)}")
    print(f"éªŒè¯é›†å¤§å°: {len(val_subset)}")

    return train_loader, val_loader, num_classes, labels

# åˆ›å»ºæ¨¡å‹
def create_model(num_classes):
    # ä½¿ç”¨é¢„è®­ç»ƒçš„ResNet50æ¨¡å‹
    model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)

    # å†»ç»“éƒ¨åˆ†å±‚ä»¥åŠ é€Ÿè®­ç»ƒ
    # é’ˆå¯¹M3 Proï¼Œåªå†»ç»“å‰é¢çš„å±‚ä»¥å……åˆ†åˆ©ç”¨èŠ¯ç‰‡æ€§èƒ½
    trainable_layer_count = 30
    print(f"å†»ç»“ResNet50å‰é¢å±‚ï¼Œä¿ç•™å{trainable_layer_count}å±‚ç”¨äºè®­ç»ƒ")
    for param in list(model.parameters())[:-trainable_layer_count]:
        param.requires_grad = False

    # ä¿®æ”¹æœ€åçš„å…¨è¿æ¥å±‚ä»¥é€‚åº”æˆ‘ä»¬çš„åˆ†ç±»ä»»åŠ¡
    model.fc = nn.Sequential(
        nn.Dropout(0.5),
        nn.Linear(model.fc.in_features, num_classes)
    )

    # æ¨¡å‹æ€»ç»“ä¿¡æ¯
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"ğŸ“Š æ¨¡å‹ä¿¡æ¯: ResNet50")
    print(f"  æ€»å‚æ•°æ•°é‡: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)")

    return model

# è®­ç»ƒå’ŒéªŒè¯å‡½æ•°
def train_epoch(model, train_loader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    # åˆ›å»ºè¿›åº¦æ¡
    progress_bar = ProgressBar(len(train_loader), prefix='è®­ç»ƒè¿›åº¦:', suffix='å®Œæˆ', length=30)

    for batch_idx, (inputs, labels) in enumerate(train_loader):
        inputs, labels = inputs.to(device), labels.to(device)

        # æ¸…é›¶æ¢¯åº¦
        optimizer.zero_grad()

        # å‰å‘ä¼ æ’­
        outputs = model(inputs)
        loss = criterion(outputs, labels)

        # åå‘ä¼ æ’­å’Œä¼˜åŒ–
        loss.backward()
        optimizer.step()

        # ç»Ÿè®¡
        running_loss += loss.item() * inputs.size(0)
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

        # æ›´æ–°è¿›åº¦æ¡
        progress_bar.update()

        # å®æ—¶æ˜¾ç¤ºæ‰¹æ¬¡è®­ç»ƒä¿¡æ¯
        if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(train_loader):
            current_acc = correct / total
            current_loss = running_loss / total
            print(f"  æ‰¹æ¬¡: {batch_idx+1}/{len(train_loader)} | æŸå¤±: {current_loss:.4f} | å‡†ç¡®ç‡: {current_acc:.4f}", end='\r')

    epoch_loss = running_loss / total
    epoch_acc = correct / total
    print()  # æ–°è¡Œ

    return epoch_loss, epoch_acc

def validate(model, val_loader, criterion, device):
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0

    # åˆ›å»ºè¿›åº¦æ¡
    progress_bar = ProgressBar(len(val_loader), prefix='éªŒè¯è¿›åº¦:', suffix='å®Œæˆ', length=30)

    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            # å‰å‘ä¼ æ’­
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            # ç»Ÿè®¡
            running_loss += loss.item() * inputs.size(0)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.update()

    epoch_loss = running_loss / total
    epoch_acc = correct / total

    # æŒ‰ç±»åˆ«è®¡ç®—å‡†ç¡®ç‡
    class_correct = {}
    class_total = {}

    if hasattr(val_loader.dataset, 'dataset') and hasattr(val_loader.dataset.dataset, 'features'):
        class_names = val_loader.dataset.dataset.features['label'].names

        # åˆå§‹åŒ–è®¡æ•°å™¨
        for i in range(len(class_names)):
            class_correct[class_names[i]] = 0
            class_total[class_names[i]] = 0

        # é‡æ–°è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
        model.eval()
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                _, predicted = torch.max(outputs, 1)

                # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
                for i in range(len(labels)):
                    label = labels[i].item()
                    class_name = class_names[label]
                    class_total[class_name] += 1
                    if predicted[i] == labels[i]:
                        class_correct[class_name] += 1

        # è¾“å‡ºæ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
        print("\nå„ç±»åˆ«éªŒè¯å‡†ç¡®ç‡:")
        for class_name in class_names:
            if class_total[class_name] > 0:
                accuracy = class_correct[class_name] / class_total[class_name]
                print(f"  {class_name}: {accuracy:.4f} ({class_correct[class_name]}/{class_total[class_name]})")

    return epoch_loss, epoch_acc

# ç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹
def plot_training_results(train_losses, val_losses, train_accs, val_accs, save_path='training_results.png'):
    plt.figure(figsize=(15, 10))

    # è®¾ç½®ä¸­æ–‡å­—ä½“ (å¦‚æœç³»ç»Ÿæ”¯æŒ)
    try:
        plt.rcParams['font.sans-serif'] = ['SimHei']  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºä¸­æ–‡æ ‡ç­¾
        plt.rcParams['axes.unicode_minus'] = False    # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºè´Ÿå·
    except:
        print("è­¦å‘Š: æ— æ³•è®¾ç½®ä¸­æ–‡å­—ä½“ï¼Œå›¾è¡¨ä¸­çš„ä¸­æ–‡å¯èƒ½æ— æ³•æ­£ç¡®æ˜¾ç¤º")

    # æŸå¤±æ›²çº¿
    plt.subplot(2, 1, 1)
    epochs = range(1, len(train_losses) + 1)
    plt.plot(epochs, train_losses, 'bo-', label='è®­ç»ƒæŸå¤±')
    plt.plot(epochs, val_losses, 'ro-', label='éªŒè¯æŸå¤±')
    plt.xlabel('è®­ç»ƒè½®æ¬¡')
    plt.ylabel('æŸå¤±å€¼')
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.legend(loc='upper right')
    plt.title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±æ›²çº¿')

    # å‡†ç¡®ç‡æ›²çº¿
    plt.subplot(2, 1, 2)
    plt.plot(epochs, train_accs, 'g*-', label='è®­ç»ƒå‡†ç¡®ç‡')
    plt.plot(epochs, val_accs, 'm^-', label='éªŒè¯å‡†ç¡®ç‡')
    plt.xlabel('è®­ç»ƒè½®æ¬¡')
    plt.ylabel('å‡†ç¡®ç‡')
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.legend(loc='lower right')
    plt.title('è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡æ›²çº¿')

    # æ ‡è®°æœ€ä½³éªŒè¯å‡†ç¡®ç‡ç‚¹
    best_epoch = np.argmax(val_accs)
    best_acc = val_accs[best_epoch]
    plt.plot([best_epoch + 1], [best_acc], 'r*', markersize=15)
    plt.annotate(f'æœ€ä½³: {best_acc:.4f}', 
                 xy=(best_epoch + 1, best_acc),
                 xytext=(best_epoch + 1 + 2, best_acc - 0.05),
                 arrowprops=dict(facecolor='black', shrink=0.05),
                 fontsize=12)

    plt.tight_layout()
    plt.savefig(save_path, dpi=300)
    print(f"âœ… è®­ç»ƒç»“æœå›¾è¡¨å·²ä¿å­˜åˆ° {save_path}")

    # é¢å¤–ä¿å­˜æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡å›¾è¡¨
    try:
        plot_class_accuracy(save_path.replace('.png', '_classes.png'))
    except Exception as e:
        print(f"æ— æ³•ç”Ÿæˆç±»åˆ«å‡†ç¡®ç‡å›¾è¡¨: {e}")

# ä¸»è®­ç»ƒå‡½æ•°
def train_model(args):
    # å‡†å¤‡æ•°æ®
    train_loader, val_loader, num_classes, labels = prepare_data(args.batch_size, args.val_split)

    # è®¾ç½®è®¾å¤‡
    device = get_device()
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # é’ˆå¯¹M3èŠ¯ç‰‡è¾“å‡ºé¢å¤–ä¿¡æ¯
    if device.type == 'mps':
        print("ä½¿ç”¨Apple Silicon M3 Proçš„MPSåŠ é€Ÿè®­ç»ƒ")
        print(f"å†…å­˜: 18GB, å»ºè®®æ‰¹æ¬¡å¤§å°: {args.batch_size}")

    # åˆ›å»ºæ¨¡å‹
    model = create_model(num_classes)
    model = model.to(device)

    # è®¾ç½®æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=args.learning_rate)

    # ä½¿ç”¨ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œå¯¹é•¿æœŸè®­ç»ƒæ›´å‹å¥½
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs, eta_min=1e-6)

    # è®­ç»ƒå¾ªç¯
    train_losses, val_losses = [], []
    train_accs, val_accs = [], []
    final_model_path = args.model_path.replace('.pth', '_final.pth')

    print("\n" + "="*50)
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ - æ€»è½®æ¬¡: {args.epochs}, æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print("="*50)

    # è®°å½•å¼€å§‹æ—¶é—´
    import time
    start_time = time.time()

    for epoch in range(args.epochs):
        # æ˜¾ç¤ºå½“å‰è½®æ¬¡å’Œæ—¶é—´
        current_time = time.strftime("%H:%M:%S", time.localtime())
        print(f"\nğŸ“Š è½®æ¬¡ {epoch+1}/{args.epochs} [å¼€å§‹æ—¶é—´: {current_time}]")

        # è®­ç»ƒä¸€ä¸ªå‘¨æœŸ
        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)

        # éªŒè¯
        val_loss, val_acc = validate(model, val_loader, criterion, device)

        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()

        # ä¿å­˜ç»Ÿè®¡æ•°æ®
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_accs.append(train_acc)
        val_accs.append(val_acc)

        # è®¡ç®—å‰©ä½™æ—¶é—´
        elapsed_time = time.time() - start_time
        avg_time_per_epoch = elapsed_time / (epoch + 1)
        remaining_epochs = args.epochs - (epoch + 1)
        estimated_time = avg_time_per_epoch * remaining_epochs
        hours, remainder = divmod(estimated_time, 3600)
        minutes, seconds = divmod(remainder, 60)

        # æ ¼å¼åŒ–æ˜¾ç¤ºæ—¶é—´
        remaining_time = f"{int(hours)}å°æ—¶ {int(minutes)}åˆ†é’Ÿ {int(seconds)}ç§’"

        # æ‰“å°æœ¬è½®æ¬¡æ€»ç»“
        print(f"\nğŸ“ è½®æ¬¡æ€»ç»“:") 
        print(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.4f}")
        print(f"  éªŒè¯æŸå¤±: {val_loss:.4f}, éªŒè¯å‡†ç¡®ç‡: {val_acc:.4f}")
        print(f"  å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.6f}")
        print(f"  é¢„è®¡å‰©ä½™æ—¶é—´: {remaining_time}")

        # æ¯10ä¸ªepochä¿å­˜ä¸€æ¬¡æ¨¡å‹
        if (epoch + 1) % 10 == 0:
            checkpoint_path = args.model_path.replace('.pth', f'_epoch{epoch+1}.pth')
            save_model(model, labels, checkpoint_path)
            print(f"ğŸ’¾ å·²ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹åˆ° {checkpoint_path} (éªŒè¯å‡†ç¡®ç‡: {val_acc:.4f})")

        # å®æ—¶ç»˜åˆ¶å¹¶ä¿å­˜è®­ç»ƒæ›²çº¿
        if (epoch + 1) % 5 == 0 or (epoch + 1) == args.epochs:
            tmp_plot_path = f"training_progress_epoch{epoch+1}.png"
            plot_training_results(train_losses, val_losses, train_accs, val_accs, save_path=tmp_plot_path)
            print(f"ğŸ“ˆ å·²æ›´æ–°è®­ç»ƒè¿›åº¦å›¾è¡¨: {tmp_plot_path}")

    # è®¡ç®—è®­ç»ƒæ€»æ—¶é—´
    train_time = time.time() - start_time
    hours, remainder = divmod(train_time, 3600)
    minutes, seconds = divmod(remainder, 60)
    time_str = f"{int(hours)}å°æ—¶ {int(minutes)}åˆ†é’Ÿ {int(seconds)}ç§’"

    print("\n" + "="*50)
    print(f"ğŸ‰ è®­ç»ƒå®Œæˆ! æ€»ç”¨æ—¶: {time_str}")

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    save_model(model, labels, final_model_path)
    print(f"ğŸ’¾ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ° {final_model_path}")

    # ç»˜åˆ¶è®­ç»ƒç»“æœ
    results_path = 'training_results.png'
    plot_training_results(train_losses, val_losses, train_accs, val_accs, save_path=results_path)

    # æ‰¾å‡ºæœ€ä½³éªŒè¯å‡†ç¡®ç‡
    best_epoch = np.argmax(val_accs)
    best_val_acc = val_accs[best_epoch]
    print(f"ğŸ“Š æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f} (ç¬¬ {best_epoch+1} è½®)")

    # è®¡ç®—å¹¶ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    print("\nå¼€å§‹è®¡ç®—æ··æ·†çŸ©é˜µå’Œç±»åˆ«å‡†ç¡®ç‡...")
    try:
        class_accuracies = plot_confusion_matrix(model, val_loader, labels, device)

        # è¾“å‡ºæ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
        print("\nğŸ“Š å„ç±»åˆ«å‡†ç¡®ç‡:")
        for class_name, accuracy in class_accuracies.items():
            print(f"  {class_name}: {accuracy:.4f}")
    except Exception as e:
        print(f"âŒ è®¡ç®—æ··æ·†çŸ©é˜µæ—¶å‡ºé”™: {e}")

    print("\nğŸš€ æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°å·²å…¨éƒ¨å®Œæˆ!")
    
    # ç”Ÿæˆè‹±æ–‡ç‰ˆè®­ç»ƒç»“æœå›¾è¡¨
    print("\nğŸ“Š ç”Ÿæˆè‹±æ–‡ç‰ˆè®­ç»ƒå›¾è¡¨...")
    try:
        plot_english_training_results(train_losses, train_accs, val_losses, val_accs)
        plot_english_final_summary(train_losses, train_accs, val_losses, val_accs, args.epochs)
        print("âœ… è‹±æ–‡å›¾è¡¨ç”Ÿæˆå®Œæˆ!")
    except Exception as e:
        print(f"âŒ ç”Ÿæˆè‹±æ–‡å›¾è¡¨æ—¶å‡ºé”™: {e}")
    
    print("="*50)

    return model, labels

# ç»˜åˆ¶ç±»åˆ«å‡†ç¡®ç‡
def plot_class_accuracy(save_path='class_accuracy.png'):
    # ä»model_results.npyåŠ è½½ç»“æœï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    results_file = 'model_results.npy'
    if not os.path.exists(results_file):
        print(f"æœªæ‰¾åˆ°{results_file}ï¼Œæ— æ³•ç»˜åˆ¶ç±»åˆ«å‡†ç¡®ç‡å›¾è¡¨")
        return

    # åŠ è½½ç»“æœ
    results = np.load(results_file, allow_pickle=True).item()
    class_names = results.get('class_names', [])
    class_accuracies = results.get('class_accuracies', {})

    if not class_accuracies:
        print("æœªæ‰¾åˆ°ç±»åˆ«å‡†ç¡®ç‡æ•°æ®")
        return

    # åˆ›å»ºå›¾è¡¨
    plt.figure(figsize=(12, 6))

    # è®¾ç½®ä¸­æ–‡å­—ä½“
    try:
        plt.rcParams['font.sans-serif'] = ['SimHei']  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºä¸­æ–‡æ ‡ç­¾
        plt.rcParams['axes.unicode_minus'] = False    # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºè´Ÿå·
    except:
        print("è­¦å‘Š: æ— æ³•è®¾ç½®ä¸­æ–‡å­—ä½“")

    # æ’åºå¹¶ç»˜åˆ¶å‡†ç¡®ç‡æ¡å½¢å›¾
    classes = list(class_accuracies.keys())
    accuracies = [class_accuracies[cls] for cls in classes]

    # é¢œè‰²æ˜ å°„
    colors = plt.cm.viridis(np.linspace(0, 1, len(classes)))

    # ç»˜åˆ¶æ¡å½¢å›¾
    bars = plt.bar(classes, accuracies, color=colors, alpha=0.8)

    # åœ¨æ¡å½¢ä¸Šæ–¹æ˜¾ç¤ºå…·ä½“æ•°å€¼
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{height:.4f}', ha='center', va='bottom', rotation=0)

    plt.xlabel('ç±»åˆ«')
    plt.ylabel('å‡†ç¡®ç‡')
    plt.title('å„ç±»åˆ«éªŒè¯å‡†ç¡®ç‡')
    plt.ylim(0, 1.1)  # ç¡®ä¿yè½´èŒƒå›´åˆé€‚
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.tight_layout()

    # ä¿å­˜å›¾è¡¨
    plt.savefig(save_path, dpi=300)
    print(f"âœ… ç±»åˆ«å‡†ç¡®ç‡å›¾è¡¨å·²ä¿å­˜åˆ° {save_path}")

# ç»˜åˆ¶æ··æ·†çŸ©é˜µ
def plot_confusion_matrix(model, data_loader, class_names, device, save_path='confusion_matrix.png'):
    from sklearn.metrics import confusion_matrix
    import seaborn as sns

    print("\nè®¡ç®—æ··æ·†çŸ©é˜µ...")

    # æ”¶é›†æ‰€æœ‰é¢„æµ‹å’ŒçœŸå®æ ‡ç­¾
    y_true = []
    y_pred = []

    # åˆ›å»ºè¿›åº¦æ¡ (ProgressBarå·²åœ¨é¡¶éƒ¨å¯¼å…¥)
    progress_bar = ProgressBar(len(data_loader), prefix='é¢„æµ‹è¿›åº¦:', suffix='å®Œæˆ', length=30)

    model.eval()
    with torch.no_grad():
        for inputs, labels in data_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)

            y_true.extend(labels.cpu().numpy())
            y_pred.extend(predicted.cpu().numpy())

            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.update()

    # è®¡ç®—æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_true, y_pred)

    # åˆ›å»ºå›¾è¡¨
    plt.figure(figsize=(10, 8))

    # è®¾ç½®ä¸­æ–‡å­—ä½“
    try:
        plt.rcParams['font.sans-serif'] = ['SimHei']  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºä¸­æ–‡æ ‡ç­¾
        plt.rcParams['axes.unicode_minus'] = False    # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºè´Ÿå·
    except:
        print("è­¦å‘Š: æ— æ³•è®¾ç½®ä¸­æ–‡å­—ä½“")

    # ç»˜åˆ¶çƒ­å›¾
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.title('æ··æ·†çŸ©é˜µ')
    plt.tight_layout()

    # ä¿å­˜å›¾è¡¨
    plt.savefig(save_path, dpi=300)
    print(f"âœ… æ··æ·†çŸ©é˜µå›¾è¡¨å·²ä¿å­˜åˆ° {save_path}")

    # ä¿å­˜æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
    class_accuracies = {}
    for i in range(len(class_names)):
        class_accuracies[class_names[i]] = cm[i, i] / np.sum(cm[i])

    # ä¿å­˜ç»“æœ
    np.save('model_results.npy', {
        'confusion_matrix': cm,
        'class_names': class_names,
        'class_accuracies': class_accuracies
    })

    return class_accuracies

# ä¸»å‡½æ•°
def main():
    parser = argparse.ArgumentParser(description='åƒåœ¾åˆ†ç±»æ¨¡å‹è®­ç»ƒ')
    parser.add_argument('--batch_size', type=int, default=64, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--epochs', type=int, default=40, help='è®­ç»ƒå‘¨æœŸæ•°')
    parser.add_argument('--learning_rate', type=float, default=0.001, help='å­¦ä¹ ç‡')
    parser.add_argument('--val_split', type=float, default=0.2, help='éªŒè¯é›†æ¯”ä¾‹')
    parser.add_argument('--model_path', type=str, default='trash_classifier.pth', help='æ¨¡å‹ä¿å­˜è·¯å¾„')

    args = parser.parse_args()

    # æ£€æŸ¥æ¨¡å‹ä¿å­˜ç›®å½•æ˜¯å¦å­˜åœ¨
    model_dir = os.path.dirname(args.model_path)
    if model_dir and not os.path.exists(model_dir):
        os.makedirs(model_dir)

    # å¼€å§‹è®­ç»ƒ
    train_model(args)

if __name__ == '__main__':
    main()
